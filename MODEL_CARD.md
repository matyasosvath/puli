# **Model Details**

Hungarian Research Centre for Linguistics developed and released the Puli family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from ~400 million to ~7 billion parameters. Our fine-tuned LLMs, called Parancs PULI and PULI Llumix 32k Instruct, are optimized for dialogue use cases.

**Model Developers** Hungarian Research Centre for Linguistics

**Input** Models input text only.

**Output** Models generate text only.

**Model Architecture** PULI models are auto-regressive language models that uses transformer architecture. The tuned versions use supervised fine-tuning (SFT).


||Training Data|Params|Context Length|Words|
|---|---|---|---|---|
PULI 2 |*A collection of publicly and privately available data*|435M|1024|32.2B


## **References**

1. Yang Zijian Győző, Dodé Réka, Ferenczi Gergő, and Váradi Tamás. “Jönnek a nagyok! BERT-Large, GPT-2 és GPT-3 nyelvmodellek magyar nyelvre.” Szeged, 2023.
